<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="https://dontfollowmeimcrazy.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dontfollowmeimcrazy.github.io/" rel="alternate" type="text/html" /><updated>2020-08-27T21:32:53+02:00</updated><id>https://dontfollowmeimcrazy.github.io/feed.xml</id><title type="html">orso.io</title><subtitle>Hello! This is the den of an atypical bear named Matteo Dunnhofer. </subtitle><entry><title type="html">New paper at ECCV 2020</title><link href="https://dontfollowmeimcrazy.github.io/2020/08/27/new-paper-at-eccv-2020/" rel="alternate" type="text/html" title="New paper at ECCV 2020" /><published>2020-08-27T00:00:00+02:00</published><updated>2020-08-27T00:00:00+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2020/08/27/new-paper-at-eccv-2020</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2020/08/27/new-paper-at-eccv-2020/">&lt;p&gt;Our newest paper &lt;strong&gt;â€œAn Exploration of Target-Conditioned Segmentation Methods for Visual Object Trackersâ€&lt;/strong&gt; is going to be presented on August 28th at the &lt;a href=&quot;https://www.votchallenge.net/vot2020/&quot;&gt;Visual Object Tracking Challenge VOT2020&lt;/a&gt; workshop, held in conjunction with the &lt;a href=&quot;https://eccv2020.eu&quot;&gt;European Conference on Computer Vision (ECCV) 2020&lt;/a&gt;. In this work, we propse an analysis of the most recent segmentation methods that are conditioned on a particular object, in order to transform any bounding-box tracker into a segmentation tracker.&lt;/p&gt;

&lt;p&gt;Here is the abstract:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Visual object tracking is the problem of predicting a target objectâ€™s state in a video. Generally, bounding-boxes have been used to represent states, and a surge of effort has been spent by the community to produce efficient causal algorithms capable of locating targets with such representations. As the field is moving towards binary segmentation masks to define objects more precisely, in this paper we propose to extensively explore target-conditioned segmentation methods available in the computer vision community, in order to transform any bounding-box tracker into a segmentation tracker. Our analysis shows that such methods allow trackers to compete with recently proposed segmentation trackers, while performing quasi real-time.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;and in the following you can have a look to a teaser video ofÂ our study (follow the links in the description for a longer presentation and qualitative examples).&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/watch?v=Gcmiv4s1J8w&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="deep learning" /><category term="visual tracking" /><category term="vot2020" /><category term="eccv2020" /><category term="uniud" /><summary type="html">Our newest paper â€œAn Exploration of Target-Conditioned Segmentation Methods for Visual Object Trackersâ€ is going to be presented on August 28th at the Visual Object Tracking Challenge VOT2020 workshop, held in conjunction with the European Conference on Computer Vision (ECCV) 2020. In this work, we propse an analysis of the most recent segmentation methods that are conditioned on a particular object, in order to transform any bounding-box tracker into a segmentation tracker.</summary></entry><entry><title type="html">First journal paper published</title><link href="https://dontfollowmeimcrazy.github.io/2020/01/15/first-journal-paper-published/" rel="alternate" type="text/html" title="First journal paper published" /><published>2020-01-15T13:26:26+01:00</published><updated>2020-01-15T13:26:26+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2020/01/15/first-journal-paper-published</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2020/01/15/first-journal-paper-published/">&lt;p&gt;I amÂ happy toÂ tell you that my firstÂ journal paper &lt;strong&gt;â€œSiam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound imagesâ€&lt;/strong&gt; is going to be published in the volume 60 of &lt;a href=&quot;https://www.journals.elsevier.com/medical-image-analysis&quot;&gt;Medical Image Analysis&lt;/a&gt;. You can find the manuscript at &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1361841519301677&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is the abstract of the paper:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The tracking of the knee femoral condyle cartilage during ultrasound-guided minimally invasive procedures is important to avoid damaging this structure during such interventions. In this study, we propose a new deep learning method to track, accurately and efficiently, the femoral condyle cartilage in ultrasound sequences, which were acquired under several clinical conditions, mimicking realistic surgical setups. Our solution, that we name Siam-U-Net, requires minimal user initialization and combines a deep learning segmentation method with a siamese framework for tracking the cartilage in temporal and spatio-temporal sequences of 2D ultrasound images. Through extensive performance validation given by the Dice Similarity Coefficient, we demonstrate that our algorithm is able to track the femoral condyle cartilage with an accuracy which is comparable to experienced surgeons. It is additionally shown that the proposed method outperforms state-of-the-art segmentation models and trackers in the localization of the cartilage. We claim that the proposed solution has the potential for ultrasound guidance in minimally invasive knee procedures.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;and in the following video you can have a look to a short recap ofÂ our study.&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/nYTlEyMoj0s&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="deep learning" /><category term="knee cartilage" /><category term="medical image analysis" /><category term="siam-u-net" /><category term="tracking" /><category term="ultrasound" /><category term="uniud" /><category term="qut" /><summary type="html">I amÂ happy toÂ tell you that my firstÂ journal paper â€œSiam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound imagesâ€ is going to be published in the volume 60 of Medical Image Analysis. You can find the manuscript at this link.</summary></entry><entry><title type="html">First paper accepted at the VOT2019 Challenge Workshop</title><link href="https://dontfollowmeimcrazy.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop/" rel="alternate" type="text/html" title="First paper accepted at the VOT2019 Challenge Workshop" /><published>2019-09-27T18:31:44+02:00</published><updated>2019-09-27T18:31:44+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop/">&lt;p&gt;I am excited to announce that my very first first-author paper titled &lt;strong&gt;â€œVisual Tracking by means of Deep Reinforcement Learning and an Expert Demonstratorâ€&lt;/strong&gt; was accepted at the &lt;a href=&quot;http://www.votchallenge.net/vot2019/index.html&quot;&gt;Visual Object Tracking (VOT) 2019 Challenge&lt;/a&gt;. This workshop is the annual premier event in the visual tracking panorama and this year it will be held in conjunction with the &lt;a href=&quot;http://iccv2019.thecvf.com&quot;&gt;International Conference on Computer Vision (ICCV) 2019&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is the abstract of the paper:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the last decade many different algorithms have been proposed to track a generic object in videos. Their execution on recent large-scale video datasets can produce a great amount of various tracking behaviours. New trends in Reinforcement Learning showed that demonstrations of an expert agent can be efficiently used to speed-up the process of policy learning. Taking inspiration from such works and from the recent applications of Reinforcement Learning to visual tracking, we propose two novel trackers, A3CT, which exploits demonstrations of a state-of-the-art tracker to learn an effective tracking policy, and A3CTD, that takes advantage of the same expert tracker to correct its behaviour during tracking. Through an extensive experimental validation on the GOT-10k, OTB-100, LaSOT, UAV123 and VOT benchmarks, we show that the proposed trackers achieve state-of-the-art performance while running in real-time.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The preprint can be found on &lt;a href=&quot;https://arxiv.org/abs/1909.08487&quot;&gt;arXiv&lt;/a&gt;. Here belowÂ you can have a look to someÂ videos where we show the performance of our proposed trackers.&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/jSGLafk4-G4&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="a3ct" /><category term="a3ctd" /><category term="deep learning" /><category term="deep reinforcement learning" /><category term="iccv" /><category term="visual tracking" /><category term="vot" /><summary type="html">I am excited to announce that my very first first-author paper titled â€œVisual Tracking by means of Deep Reinforcement Learning and an Expert Demonstratorâ€ was accepted at the Visual Object Tracking (VOT) 2019 Challenge. This workshop is the annual premier event in the visual tracking panorama and this year it will be held in conjunction with the International Conference on Computer Vision (ICCV) 2019.</summary></entry><entry><title type="html">bandidos en barcelona</title><link href="https://dontfollowmeimcrazy.github.io/2017/11/29/bandidos-en-barcelona/" rel="alternate" type="text/html" title="bandidos en barcelona" /><published>2017-11-29T22:38:02+01:00</published><updated>2017-11-29T22:38:02+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/11/29/bandidos-en-barcelona</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/11/29/bandidos-en-barcelona/">&lt;p&gt;I finally had the time to edit the clips that my friends and I filmed during our skate-trip to Barcelona two months ago. Do not expect big tricks, we wanted just to have fun cruising the around the streets. We had very good times, although the Catalonian crisis, people was still really friendly and smiling. Barcelona is really an amazing city!&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/2C-LwgQEsgw&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><summary type="html">I finally had the time to edit the clips that my friends and I filmed during our skate-trip to Barcelona two months ago. Do not expect big tricks, we wanted just to have fun cruising the around the streets. We had very good times, although the Catalonian crisis, people was still really friendly and smiling. Barcelona is really an amazing city!</summary></entry><entry><title type="html">2007-2017</title><link href="https://dontfollowmeimcrazy.github.io/2017/10/02/2007-2017/" rel="alternate" type="text/html" title="2007-2017" /><published>2017-10-02T14:41:32+02:00</published><updated>2017-10-02T14:41:32+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/10/02/2007-2017</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/10/02/2007-2017/">&lt;p&gt;This year my crew and I celebrate our first ten years on the skateboard. For the occasion we decided to produce some skateboard decks and so I had to take back the pencil and start drawing (like in the good old days). In the drawing it is represented, in a certain way, our â€œskateboarding pathâ€ from where we began to where we are now. You can see the result above, and the final printed board below. We are very happy with them.&lt;/p&gt;

&lt;p&gt;Now we go skate!&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-10-02/v1_cropgg.jpg&quot; /&gt;
&lt;/div&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-10-02/IMG_3736.jpg&quot; /&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="10 years" /><category term="pine" /><category term="skate" /><category term="skateboarding" /><category term="tarvisio" /><summary type="html">This year my crew and I celebrate our first ten years on the skateboard. For the occasion we decided to produce some skateboard decks and so I had to take back the pencil and start drawing (like in the good old days). In the drawing it is represented, in a certain way, our â€œskateboarding pathâ€ from where we began to where we are now. You can see the result above, and the final printed board below. We are very happy with them.</summary></entry><entry><title type="html">My first Kaggle competition</title><link href="https://dontfollowmeimcrazy.github.io/2017/08/04/my-first-kaggle-competition/" rel="alternate" type="text/html" title="My first Kaggle competition" /><published>2017-08-05T00:08:44+02:00</published><updated>2017-08-05T00:08:44+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/08/04/my-first-kaggle-competition</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/08/04/my-first-kaggle-competition/">&lt;p&gt;I knew of the existence of Kaggle since I started studying Machine Learning. Unfortunately, during that time I did not have the experience nor the time to take part to any competition. But in last semester I made some good experiences that helped me to develop my skills in the field. AtÂ the same time, an interesting real world problemÂ was published as a &lt;a href=&quot;https://www.kaggle.com/c/planet-understanding-the-amazon-from-space&quot;&gt;competition on the Â Kaggle platform&lt;/a&gt;. And so I realisedÂ that that was the best one to giveÂ myself a challenge.&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-08-04/habitation1.jpg&quot; /&gt;
  
  &lt;p class=&quot;caption&quot;&gt;
    An example of satellite image used in the competition. Credits: Planet and Kaggle.
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://www.planet.com&quot;&gt;Planet Labs&lt;/a&gt;Â is a private company that owns the multitude ofÂ imaging-satellites that orbit around the Earth. These satellites form a constellation that can provide a complete image of our planet, all the time. This great amount of data is used by governments and Â humanitarian, business, environmental (and many more) organisations to monitorÂ the state and the changes of Earthâ€™s ground. These stakeholdersÂ can also use the images to study the evolution or track the variationsÂ in different (maybe critic) areas of the planet, and exactly that is the context of the competition organised by Planet and its Brazilian partner SCCON: the deforestation of the Amazon rainforest.Â The seriousnessÂ of this problem (and it is really serious!) obviously grabs the attention of many researchers that use satellite data to track the rapid changes that occur inside the forest. But often its the huge vastness make the process of analysing the images really tedious, and a system that could automatically identify deforestation risk zones will certainly benefit the work of these people. These kind of procedures already exist but they do not for the images produced by Planet norÂ at an high level of resolution. So, Planet decided to challenge Kaggleâ€™s users with the goal to develop a software tool able to classify the atmospheric condition and ground characteristics contained inside satellite images of the Amazon.&lt;/p&gt;

&lt;p&gt;The datasetÂ provided consisted of more than 40000 training examples. Every exampleÂ (a chip image) was a 256px x 256px GeoTiff 4-bandÂ (red, green, blue, near infrared) image that covered anÂ area of there forest ofÂ circa 90 hectares (950mt x 950mt circa). The associated labels explicatedÂ the weather conditionÂ (clear, cloudy, partly cloudy, haze) and the characteristics of the ground in that particularÂ area (primary rainforest, water, habitation, agricolture, cultivation, road, bare ground, slash-and-burn cultivation, selective logging, blooming trees, conventional mines, artisanal mines and blow down). Every sample was classified with exactly one weather label and at least one (at most all) label for the ground. So, this happened to be a multi-label multi-class classification problem, instead of a classicÂ multi-class classification. The labels were very unbalanced, making some classes really difficult to predict. Moreover, together with theÂ GeoTiff images, Planet provided theirÂ RGB version for practice. Finally, the test set contained 61191 samplesÂ which labels had to be predicted. The metric used by Kaggle for the evaluation was the F2 score.&lt;/p&gt;

&lt;p&gt;Given this good amount of training data, the use of Convolutional Neural Network (today the standard approach in Computer Vision) is very natural. To produce my solution I implemented different models of CNN, and &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt; andÂ &lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;DenseNet&lt;/a&gt; (the latest ImageNetâ€™s competition winners) were the ones that performed best. I trained both from-scratch and pretrained nets, the latter givingÂ some improvements in the score. Although ImageNet does not contain this kind of satellite images, the features produced by pretrained models wereÂ more meaningful.Â In all the models, the last activation (usually a softmax) was replaced with a sigmoid. This was done to compute every class probability independently from the other classes, and so I could use labels as one-hot vectors with many ones as the number of classes present in the image. Classic tricks like data augmentation and normalisation, learning rate scheduling etc were performed.&lt;/p&gt;

&lt;p&gt;The aim of Planet was to obtain models built on the GeoTiff images. Turned out from the experiments that the RGB images (provided only for practice) were more significant for the nets, and models trained on them performed definitelyÂ better than the one trained on the 4-band images. This was caused by a lot misclassifications in the latter.&lt;/p&gt;

&lt;p&gt;For the predictions on the test set, instead of using the output of the single models, I implemented an ensemble of the best performing nets (that I evaluated on a validation set produced splitting the training set). In particular, I madeÂ ResNet34, ResNet101, DenseNet121, DenseNet169 and DenseNet201 vote for the labels of every test example. Then, classes that obtained a vote greater of equal to half of the number of models (in this case if a label was voted by at least three models) were retained. Before that, I also optimised the threshold of the class probabilities (through a brute force search), meaning that a class was predictedÂ if its probability was greater than a particular value.&lt;/p&gt;

&lt;p&gt;All the sources were developedÂ in Python. For the models trained from scratch I used TensorFlow, to use the pretrained nets I employed PyTorch.&lt;/p&gt;

&lt;p&gt;My solutionÂ obtained a final F2 score of 0.92942 that earned me the 88th position (inside the top 10%) and the bronze medal.&lt;/p&gt;

&lt;p&gt;In conclusion, I am very happy for this first result and itÂ has been a great and exciting experience from which a learned a lot of things. Among the others, it allowed me to study and implement complex and very effective CNNs, to face real world datasets (and they are not as the notorious you find in papers), to learnÂ how ensembling canÂ give a boost to prediction, and to discover the flexibility and simplicity ofÂ PyTorch.&lt;/p&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="Amazon" /><category term="deep learning" /><category term="deforestation" /><category term="Kaggle" /><category term="machine learning" /><category term="rainforest" /><summary type="html">I knew of the existence of Kaggle since I started studying Machine Learning. Unfortunately, during that time I did not have the experience nor the time to take part to any competition. But in last semester I made some good experiences that helped me to develop my skills in the field. AtÂ the same time, an interesting real world problemÂ was published as a competition on the Â Kaggle platform. And so I realisedÂ that that was the best one to giveÂ myself a challenge.</summary></entry><entry><title type="html">ASD Forest Park Crew membership registration</title><link href="https://dontfollowmeimcrazy.github.io/2017/06/20/asd-forest-park-crew-membership-registration/" rel="alternate" type="text/html" title="ASD Forest Park Crew membership registration" /><published>2017-06-20T13:54:49+02:00</published><updated>2017-06-20T13:54:49+02:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/06/20/asd-forest-park-crew-membership-registration</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/06/20/asd-forest-park-crew-membership-registration/">&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;During last spring I had the opportunity to do a little work for the ASD Forest Park Crew association (of which I am a member) that runs my hometown skatepark.&lt;/p&gt;

&lt;p&gt;To access our skateparkÂ a person must be a member of our association. Until last year, to become a fellow a person had to present himself at the skatepark, pay the registration fee and then get a paper card valid for the access to the park. A classic registration scenario. Starting fromÂ this seasonÂ we decided to automate and digitize all this process. So, I realized a little web-app to manage the registration, the payment via PayPal and the generation of a PDF file that countsÂ as a valid pass for the skatepark.Â With this â€œinnovationâ€ we are able to manage electronically and efficiently all the data about the associationâ€™s fellows and to reduce some managing costs.&lt;/p&gt;

&lt;p&gt;You can check the appÂ at this &lt;a href=&quot;http://orso.io/forestpark/registration&quot;&gt;page&lt;/a&gt;. For every kind of info or to report a malfunction please send me an E-Mail.&lt;/p&gt;

&lt;p&gt;Weâ€™ll wait for you at the skatepark ğŸ˜‰.&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-06-20/park.jpg&quot; /&gt;
  
  &lt;p class=&quot;caption&quot;&gt;
    Forest Park skatepark in Tarvisio.
  &lt;/p&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="Forest Park" /><category term="registration" /><category term="Skateboard" /><category term="skatepark" /><category term="tarvisio" /><summary type="html">Hello everyone!</summary></entry><entry><title type="html">A first look to ImageNet</title><link href="https://dontfollowmeimcrazy.github.io/2017/02/02/a-first-look-to-imagenet/" rel="alternate" type="text/html" title="A first look to ImageNet" /><published>2017-02-02T11:54:05+01:00</published><updated>2017-02-02T11:54:05+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2017/02/02/a-first-look-to-imagenet</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2017/02/02/a-first-look-to-imagenet/">&lt;p&gt;In the last years a lot of coverage was given to Artificial Intelligence. During the last months of my Bachelorâ€™s, reading more and more about the successful applications, I started to feelÂ really curious about that areaÂ of AI calledÂ &lt;strong&gt;Deep Learning&lt;/strong&gt;.Â Actually, Deep Learning is a sub-field of the more generalÂ &lt;strong&gt;Machine Learning&lt;/strong&gt;, that is the field of Computer Science which â€œgives a computer the ability to learn without be explicitlyÂ programmedâ€ (&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;Wikipedia&lt;/a&gt;) or, in differentÂ words,Â aims to design programs that can improve their performance by â€œlearningâ€ from examples (a better and easier definition &lt;a href=&quot;https://www.quora.com/What-is-the-best-description-you-can-give-to-the-subject-of-machine-learning-to-a-layperson-or-a-kid&quot;&gt;here&lt;/a&gt;). The theories of Deep Learning focusÂ on building Deep Neural Networks (DNN), which are complex mathematical models that loosely mimic the behaviour of the human brain. The procedure toÂ make DNNs â€œintelligentâ€; simulates the way people (and especially babies) learn: you perceive something, you make sense of it and â€œchange your mindâ€ accordingly. Really fascinating!&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-02-02/tsne-full.jpg&quot; /&gt;
  
  &lt;p class=&quot;caption&quot;&gt;
    Samples of similar images that were used to train AlexNet. They are arranged by the similarity of the abstractions they contain.
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Luckily, my university offers a Machine Learning course so I had the opportunity to study and put hands on the things I was interested. As part of a project, I was asked to implement and train a &lt;strong&gt;Convolutional Neural Network&lt;/strong&gt; (ConvNet), a special type of neural network that is usually applied to imagesÂ (they are used in your Prisma filtersÂ too!). You can think this kind of networks as the artificial counterpart of our vision system. ConvNets are composed of different layers of artificial neurons that are able to learn various levels of abstractions: for example, when applied to an image of a person, the first layers of neurons get excited when they recognise simple patterns as lines or edges; the immediate following layers use the edgesÂ to compose more complicated abstractions such as eyes, nose, mouth, hands, etc; then towards the last layers, the previous patters are merged in a full abstraction of the person. To learn this â€œinternal representationâ€ these models areÂ shownÂ just a lot if images (with their category associated) a lot of times. Today, ConvNets are the standard in Computer Vision.&lt;/p&gt;

&lt;p&gt;I implemented &lt;strong&gt;&lt;a href=&quot;http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf&quot;&gt;AlexNet&lt;/a&gt;&lt;/strong&gt;, the first model of ConvNet that was usedÂ by the scientific community to win the ILSVRC competition. IÂ trained it showing and showingÂ a set of &lt;em&gt;1.2 million images&lt;/em&gt; of &lt;em&gt;1000 different categories&lt;/em&gt; (the famousÂ &lt;strong&gt;&lt;a href=&quot;http://image-net.org/challenges/LSVRC/2012/&quot;&gt;ImageNet&lt;/a&gt;&lt;/strong&gt;dataset) for more than forty times. The training took a few days. It was stopped after the neural network reached a good level of generalisation, that is the ability to predict the category of examples that it has never seen. Once trained, if you input it with an image, it will output the top five categories that best classify the image. For example, feeding AlexNet with the image below&lt;/p&gt;

&lt;div class=&quot;post-image&quot;&gt;
  &lt;img class=&quot;post-image&quot; src=&quot;https://dontfollowmeimcrazy.github.io/assets/images/blog/2017-02-02/lussari.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;gives the output:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;AlexNet saw:
alp - score: 0.575796604156
church, church building - score: 0.0516746938229
valley, vale - score: 0.0432425364852
castle - score: 0.0284509658813
monastery - score: 0.0265731271356
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the categoriesÂ are ordered by the probability (score) that the image belongs to that class.&lt;/p&gt;

&lt;p&gt;I created a &lt;a href=&quot;https://github.com/dontfollowmeimcrazy/imagenet&quot;&gt;repository on GitHub&lt;/a&gt; with all the source codes I used to implement, train and test AlexNet. I coded them in Python using Googleâ€™s &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; framework. If you like to dive &lt;em&gt;deeper&lt;/em&gt;Â into the technical details, refer to the repo.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT 20-11-2017:&lt;/strong&gt;Â I added to the repository a version of the code using TensorFlowâ€™s new imperative style, &lt;a href=&quot;https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html&quot;&gt;TensorFlow Eager&lt;/a&gt;. Unfortunately I did not have the time nor the resources to train and test AlexNet using these new scripts, but I expect results similar to the old ones (model architecture and training pipeline remained the same). If anyone would try to take charge of this, itÂ would be really appreciated!&lt;/p&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="AlexNet" /><category term="ConvNet" /><category term="Deep Learning" /><category term="ImageNet" /><category term="Machine Learning" /><category term="TensorFlow" /><summary type="html">In the last years a lot of coverage was given to Artificial Intelligence. During the last months of my Bachelorâ€™s, reading more and more about the successful applications, I started to feelÂ really curious about that areaÂ of AI calledÂ Deep Learning.Â Actually, Deep Learning is a sub-field of the more generalÂ Machine Learning, that is the field of Computer Science which â€œgives a computer the ability to learn without be explicitlyÂ programmedâ€ (Wikipedia) or, in differentÂ words,Â aims to design programs that can improve their performance by â€œlearningâ€ from examples (a better and easier definition here). The theories of Deep Learning focusÂ on building Deep Neural Networks (DNN), which are complex mathematical models that loosely mimic the behaviour of the human brain. The procedure toÂ make DNNs â€œintelligentâ€; simulates the way people (and especially babies) learn: you perceive something, you make sense of it and â€œchange your mindâ€ accordingly. Really fascinating!</summary></entry><entry><title type="html">The Pine video</title><link href="https://dontfollowmeimcrazy.github.io/2016/12/08/the-pine-video/" rel="alternate" type="text/html" title="The Pine video" /><published>2016-12-08T18:11:44+01:00</published><updated>2016-12-08T18:11:44+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2016/12/08/the-pine-video</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2016/12/08/the-pine-video/">&lt;p&gt;On this day, two years ago, the first Pine crewâ€™s video was published on YouTube.&lt;/p&gt;

&lt;p&gt;It featured parts of Marco Sandrini, Luca Minigher, Johnny Fabiani and me.&lt;/p&gt;

&lt;p&gt;Check it!&lt;/p&gt;

&lt;div class=&quot;post-video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/OPcLZYuM6yg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="Friuli" /><category term="Skateboard" /><category term="tarvisio" /><category term="Udine" /><summary type="html">On this day, two years ago, the first Pine crewâ€™s video was published on YouTube.</summary></entry><entry><title type="html">FasterApp iOS version 1.3</title><link href="https://dontfollowmeimcrazy.github.io/2016/11/08/fasterapp-ios-version-1-3/" rel="alternate" type="text/html" title="FasterApp iOS version 1.3" /><published>2016-11-08T17:36:36+01:00</published><updated>2016-11-08T17:36:36+01:00</updated><id>https://dontfollowmeimcrazy.github.io/2016/11/08/fasterapp-ios-version-1-3</id><content type="html" xml:base="https://dontfollowmeimcrazy.github.io/2016/11/08/fasterapp-ios-version-1-3/">&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;I finally found the time to update my application, since iOS 10 came out. This latest version, which is 1.3, is &lt;a href=&quot;https://itunes.apple.com/it/app/fasterapp/id1024576070?mt=8&quot;&gt;already up&lt;/a&gt; on the AppStore.&lt;/p&gt;

&lt;p&gt;One of the greatest features of iOS 10 is the opening of Siri to developers. The things you can program Siri to answer to are still limited but I was willing to experiment them a little. So I managed to use Siriâ€™s abilities to letÂ the user start a gameplay modality just with its voice. My â€œprogrammedâ€Â phrases, that are consequentially recognised by Siri, fall in the domain of workouts and fitness. Examples are: â€œI want to play with FasterAppâ€;Â â€Start a Free mode workout with FasterAppâ€; â€œI want to run on Arcade mode with FasterAppâ€; â€œLetâ€™s take a run on a Personal track with FasterAppâ€. I hope to get these things working better in the next updates.&lt;/p&gt;

&lt;p&gt;In addition to iOS 10 support and the integration with Siri, version 1.3 of FasterApp features a new button to change the view during the gameplay. With this, you can switch between two angles of sight on the mapÂ you are playing, allowing you to better orient yourself. Finally, I made minor improvements to the user experience adding sounds, animations and better graphics.&lt;/p&gt;</content><author><name>dontfollowmeimcrazy</name></author><category term="1.3" /><category term="FasterApp" /><category term="update" /><summary type="html">Hello!</summary></entry></feed>