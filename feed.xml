<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="https://matteo-dunnhofer.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://matteo-dunnhofer.github.io/" rel="alternate" type="text/html" /><updated>2023-11-11T11:28:55+01:00</updated><id>https://matteo-dunnhofer.github.io/feed.xml</id><title type="html">Matteo Dunnhofer</title><subtitle>Hello! This is the den of an atypical bear named Matteo Dunnhofer. </subtitle><entry><title type="html">Paper published at MIDL 2021</title><link href="https://matteo-dunnhofer.github.io/2021/07/12/paper-published-at-MIDL-2021/" rel="alternate" type="text/html" title="Paper published at MIDL 2021" /><published>2021-07-12T00:00:00+02:00</published><updated>2021-07-12T00:00:00+02:00</updated><id>https://matteo-dunnhofer.github.io/2021/07/12/paper-published-at-MIDL-2021</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2021/07/12/paper-published-at-MIDL-2021/"><![CDATA[<p>I am happy to announce that our latest paper titled <strong>“Improving MRI-based Knee Disorder Diagnosis with Pyramidal Feature Details”</strong> has been presented last week at the <a href="https://2021.midl.io">Medical Imaging with Deep Learnign (MIDL) Conference 2021</a>. In this work, we proposed a new convolutional neural network (CNN) architecture to improve the automatic diagnosis of knee disorders such as the ACL and the meniscus tear via magnetic resonance imaging.</p>

<p>Here is the abstract:</p>

<p><em>This paper presents MRPyrNet, a new convolutional neural network (CNN) architecture that improves the capabilities of CNN-based pipelines for knee injury detection via magnetic resonance imaging (MRI). Existing works showed that anomalies are localized in small-sized knee regions that appear in particular areas of MRI scans. Based on such facts, MRPyrNet exploits a Feature Pyramid Network to enhance small appearing features and Pyramidal Detail Pooling to capture such relevant information in a robust way. Experimental results on two publicly available datasets demonstrate that MRPyrNet improves the ACL tear and meniscal tear diagnosis capabilities of two state-of-the-art methodologies. Code is available at https://git. io/JtMPH.</em></p>

<p>and in the following you can access some resources (link to the paper, code, conference webpage for our paper, and video presentation).</p>

<p><a href="https://openreview.net/forum?id=7psPmlNffvg">[link]</a></p>

<p><a href="https://github.com/matteo-dunnhofer/MRPyrNet">[code]</a></p>

<p><a href="https://2021.midl.io/papers/j6">[conference paper page]</a></p>

<div class="post-video">
  <iframe src="https://www.youtube.com/embed/XkHiyNDK2Xk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Research" /><category term="medical image analysis" /><category term="deep learning" /><category term="mrpyrnet" /><category term="knee disorder diagnosi" /><category term="MIDL 2021" /><category term="uniud" /><summary type="html"><![CDATA[I am happy to announce that our latest paper titled “Improving MRI-based Knee Disorder Diagnosis with Pyramidal Feature Details” has been presented last week at the Medical Imaging with Deep Learnign (MIDL) Conference 2021. In this work, we proposed a new convolutional neural network (CNN) architecture to improve the automatic diagnosis of knee disorders such as the ACL and the meniscus tear via magnetic resonance imaging.]]></summary></entry><entry><title type="html">Paper published on IEEE RA-L</title><link href="https://matteo-dunnhofer.github.io/2021/04/02/paper-published-on-IEEE-RAL/" rel="alternate" type="text/html" title="Paper published on IEEE RA-L" /><published>2021-04-02T00:00:00+02:00</published><updated>2021-04-02T00:00:00+02:00</updated><id>https://matteo-dunnhofer.github.io/2021/04/02/paper-published-on-IEEE-RAL</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2021/04/02/paper-published-on-IEEE-RAL/"><![CDATA[<p>I am very happy to announce that our latest paper titled <strong>“Weakly-Supervised Domain Adaptation of Deep Regression Trackers via Reinforced Knowledge Distillation”</strong> has been published on the <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7083369">IEEE Robotics and Automation Letters (RA-L)</a> journal. In this work, we propose a weakly-supervised strategy to adapt the generic-object tracking capabilities of deep regression trackers to particular and small data application domains. Our solution, which is based on <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Dunnhofer_Tracking-by-Trackers_with_a_Distilled_and_Reinforced_Model_ACCV_2020_paper.html">our recent framework</a>, allows such kind of trackers to become as accurate as state-of-the-art methods while being much more efficient and faster.</p>

<p>Here is the abstract:</p>

<p><em>Deep regression trackers are among the fastest tracking algorithms available, and therefore suitable for real-time robotic applications. However, their accuracy is inadequate in many domains due to distribution shift and overfitting. In this paper we overcome such limitations by presenting the first methodology for domain adaption of such a class of trackers. To reduce the labeling effort we propose a weakly-supervised adaptation strategy, in which reinforcement learning is used to express weak supervision as a scalar application-dependent and temporally-delayed feedback. At the same time, knowledge distillation is employed to guarantee learning stability and to compress and transfer knowledge from more powerful but slower trackers. Extensive experiments on five different robotic vision domains demonstrate the relevance of our methodology. Real-time speed is achieved on embedded devices and on machines without GPUs, while accuracy reaches significant results.</em></p>

<p>and in the following you can access some resources (link to the paper, preprint paper, and qualitative examples).</p>

<p><a href="https://ieeexplore.ieee.org/document/9394708">[link]</a></p>

<p><a href="https://arxiv.org/abs/2103.14496">[arXiv preprint]</a></p>

<div class="post-video">
  <iframe src="https://www.youtube.com/embed/3T3BJudDSwQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Research" /><category term="deep learning" /><category term="domain adaptation" /><category term="visual tracking" /><category term="IEEE Robotics and Automation Letters" /><category term="uniud" /><summary type="html"><![CDATA[I am very happy to announce that our latest paper titled “Weakly-Supervised Domain Adaptation of Deep Regression Trackers via Reinforced Knowledge Distillation” has been published on the IEEE Robotics and Automation Letters (RA-L) journal. In this work, we propose a weakly-supervised strategy to adapt the generic-object tracking capabilities of deep regression trackers to particular and small data application domains. Our solution, which is based on our recent framework, allows such kind of trackers to become as accurate as state-of-the-art methods while being much more efficient and faster.]]></summary></entry><entry><title type="html">Paper accepted at ACCV 2020</title><link href="https://matteo-dunnhofer.github.io/2020/10/13/paper-accepted-at-accv-2020/" rel="alternate" type="text/html" title="Paper accepted at ACCV 2020" /><published>2020-10-13T00:00:00+02:00</published><updated>2020-10-13T00:00:00+02:00</updated><id>https://matteo-dunnhofer.github.io/2020/10/13/paper-accepted-at-accv-2020</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2020/10/13/paper-accepted-at-accv-2020/"><![CDATA[<p>I’m happy to announce that our newest paper <strong>“Tracking-by-Trackers with a Distilled and Reinforced Model”</strong> has been accepted for publication by <a href="http://accv2020.kyoto">Asian Conference on Computer Vision (ACCV) 2020</a> which will be held next December. In this work, we propose a new tracking-by-trackers framework which unifies visual tracking goals that have been reasoned independently in the past.</p>

<p>Here is the abstract:</p>

<p><em>Visual object tracking was generally tackled by reasoning independently on fast processing algorithms, accurate online adaptation methods, and fusion of trackers. In this paper, we unify such goals by proposing a novel tracking methodology that takes advantage of other visual trackers, offline and online. A compact student model is trained via the marriage of knowledge distillation and reinforcement learning. The first allows to transfer and compress tracking knowledge of other trackers. The second enables the learning of evaluation measures which are then exploited online. After learning, the student can be ultimately used to build (i) a very fast single-shot tracker, (ii) a tracker with a simple and effective online adaptation mechanism, (iii) a tracker that performs fusion of other trackers. Extensive validation shows that the proposed algorithms compete with real-time state-of-the-art trackers.</em></p>

<p>and in the following you can access some resources (preprint paper, code, and spotlight presentation).</p>

<p><a href="https://arxiv.org/abs/2007.04108">[arXiv preprint]</a></p>

<p><a href="https://github.com/dontfollowmeimcrazy/vot-kd-rl">[code]</a></p>

<div class="post-video">
  <iframe src="https://www.youtube.com/embed/VzeoHQQl-rA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Research" /><category term="deep learning" /><category term="visual tracking" /><category term="accv2020" /><category term="uniud" /><summary type="html"><![CDATA[I’m happy to announce that our newest paper “Tracking-by-Trackers with a Distilled and Reinforced Model” has been accepted for publication by Asian Conference on Computer Vision (ACCV) 2020 which will be held next December. In this work, we propose a new tracking-by-trackers framework which unifies visual tracking goals that have been reasoned independently in the past.]]></summary></entry><entry><title type="html">New paper at ECCV 2020</title><link href="https://matteo-dunnhofer.github.io/2020/08/27/new-paper-at-eccv-2020/" rel="alternate" type="text/html" title="New paper at ECCV 2020" /><published>2020-08-27T00:00:00+02:00</published><updated>2020-08-27T00:00:00+02:00</updated><id>https://matteo-dunnhofer.github.io/2020/08/27/new-paper-at-eccv-2020</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2020/08/27/new-paper-at-eccv-2020/"><![CDATA[<p>Our newest paper <strong>“An Exploration of Target-Conditioned Segmentation Methods for Visual Object Trackers”</strong> is going to be presented on August 28th at the <a href="https://www.votchallenge.net/vot2020/">Visual Object Tracking Challenge VOT2020</a> workshop, held in conjunction with the <a href="https://eccv2020.eu">European Conference on Computer Vision (ECCV) 2020</a>. In this work, we propose an analysis of the most recent segmentation methods that are conditioned on a particular object, in order to transform any bounding-box tracker into a segmentation tracker.</p>

<p>Here is the abstract:</p>

<p><em>Visual object tracking is the problem of predicting a target object’s state in a video. Generally, bounding-boxes have been used to represent states, and a surge of effort has been spent by the community to produce efficient causal algorithms capable of locating targets with such representations. As the field is moving towards binary segmentation masks to define objects more precisely, in this paper we propose to extensively explore target-conditioned segmentation methods available in the computer vision community, in order to transform any bounding-box tracker into a segmentation tracker. Our analysis shows that such methods allow trackers to compete with recently proposed segmentation trackers, while performing quasi real-time.</em></p>

<p>and in the following you can have a look to a teaser video of our study (follow the links in the description for the arXiv preprint, a longer presentation, and qualitative examples).</p>

<div class="post-video">
  <iframe src="https://www.youtube.com/embed/Gcmiv4s1J8w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Research" /><category term="deep learning" /><category term="visual tracking" /><category term="vot2020" /><category term="eccv2020" /><category term="uniud" /><summary type="html"><![CDATA[Our newest paper “An Exploration of Target-Conditioned Segmentation Methods for Visual Object Trackers” is going to be presented on August 28th at the Visual Object Tracking Challenge VOT2020 workshop, held in conjunction with the European Conference on Computer Vision (ECCV) 2020. In this work, we propose an analysis of the most recent segmentation methods that are conditioned on a particular object, in order to transform any bounding-box tracker into a segmentation tracker.]]></summary></entry><entry><title type="html">First journal paper published</title><link href="https://matteo-dunnhofer.github.io/2020/01/15/first-journal-paper-published/" rel="alternate" type="text/html" title="First journal paper published" /><published>2020-01-15T13:26:26+01:00</published><updated>2020-01-15T13:26:26+01:00</updated><id>https://matteo-dunnhofer.github.io/2020/01/15/first-journal-paper-published</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2020/01/15/first-journal-paper-published/"><![CDATA[<p>I am happy to tell you that my first journal paper <strong>“Siam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound images”</strong> is going to be published in the volume 60 of <a href="https://www.journals.elsevier.com/medical-image-analysis">Medical Image Analysis</a>. You can find the manuscript at <a href="https://www.sciencedirect.com/science/article/pii/S1361841519301677">this link</a>.</p>

<p>Here is the abstract of the paper:</p>

<p><em>The tracking of the knee femoral condyle cartilage during ultrasound-guided minimally invasive procedures is important to avoid damaging this structure during such interventions. In this study, we propose a new deep learning method to track, accurately and efficiently, the femoral condyle cartilage in ultrasound sequences, which were acquired under several clinical conditions, mimicking realistic surgical setups. Our solution, that we name Siam-U-Net, requires minimal user initialization and combines a deep learning segmentation method with a siamese framework for tracking the cartilage in temporal and spatio-temporal sequences of 2D ultrasound images. Through extensive performance validation given by the Dice Similarity Coefficient, we demonstrate that our algorithm is able to track the femoral condyle cartilage with an accuracy which is comparable to experienced surgeons. It is additionally shown that the proposed method outperforms state-of-the-art segmentation models and trackers in the localization of the cartilage. We claim that the proposed solution has the potential for ultrasound guidance in minimally invasive knee procedures.</em></p>

<p>and in the following video you can have a look to a short recap of our study.</p>

<div class="post-video">
  <iframe src="https://www.youtube.com/embed/nYTlEyMoj0s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Research" /><category term="deep learning" /><category term="knee cartilage" /><category term="medical image analysis" /><category term="siam-u-net" /><category term="tracking" /><category term="ultrasound" /><category term="uniud" /><category term="qut" /><summary type="html"><![CDATA[I am happy to tell you that my first journal paper “Siam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound images” is going to be published in the volume 60 of Medical Image Analysis. You can find the manuscript at this link.]]></summary></entry><entry><title type="html">First paper accepted at the VOT2019 Challenge Workshop</title><link href="https://matteo-dunnhofer.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop/" rel="alternate" type="text/html" title="First paper accepted at the VOT2019 Challenge Workshop" /><published>2019-09-27T18:31:44+02:00</published><updated>2019-09-27T18:31:44+02:00</updated><id>https://matteo-dunnhofer.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2019/09/27/first-paper-accepted-at-the-vot2019-challenge-workshop/"><![CDATA[<p>I am excited to announce that my very first first-author paper titled <strong>“Visual Tracking by means of Deep Reinforcement Learning and an Expert Demonstrator”</strong> was accepted at the <a href="http://www.votchallenge.net/vot2019/index.html">Visual Object Tracking (VOT) 2019 Challenge</a>. This workshop is the annual premier event in the visual tracking panorama and this year it will be held in conjunction with the <a href="http://iccv2019.thecvf.com">International Conference on Computer Vision (ICCV) 2019</a>.</p>

<p>Here is the abstract of the paper:</p>

<p><em>In the last decade many different algorithms have been proposed to track a generic object in videos. Their execution on recent large-scale video datasets can produce a great amount of various tracking behaviours. New trends in Reinforcement Learning showed that demonstrations of an expert agent can be efficiently used to speed-up the process of policy learning. Taking inspiration from such works and from the recent applications of Reinforcement Learning to visual tracking, we propose two novel trackers, A3CT, which exploits demonstrations of a state-of-the-art tracker to learn an effective tracking policy, and A3CTD, that takes advantage of the same expert tracker to correct its behaviour during tracking. Through an extensive experimental validation on the GOT-10k, OTB-100, LaSOT, UAV123 and VOT benchmarks, we show that the proposed trackers achieve state-of-the-art performance while running in real-time.</em></p>

<p>The preprint can be found on <a href="https://arxiv.org/abs/1909.08487">arXiv</a>. Here below you can have a look to some videos where we show the performance of our proposed trackers.</p>

<div class="post-video">
  <iframe src="https://www.youtube.com/embed/jSGLafk4-G4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Research" /><category term="a3ct" /><category term="a3ctd" /><category term="deep learning" /><category term="deep reinforcement learning" /><category term="iccv" /><category term="visual tracking" /><category term="vot" /><summary type="html"><![CDATA[I am excited to announce that my very first first-author paper titled “Visual Tracking by means of Deep Reinforcement Learning and an Expert Demonstrator” was accepted at the Visual Object Tracking (VOT) 2019 Challenge. This workshop is the annual premier event in the visual tracking panorama and this year it will be held in conjunction with the International Conference on Computer Vision (ICCV) 2019.]]></summary></entry><entry><title type="html">bandidos en barcelona</title><link href="https://matteo-dunnhofer.github.io/2017/11/29/bandidos-en-barcelona/" rel="alternate" type="text/html" title="bandidos en barcelona" /><published>2017-11-29T22:38:02+01:00</published><updated>2017-11-29T22:38:02+01:00</updated><id>https://matteo-dunnhofer.github.io/2017/11/29/bandidos-en-barcelona</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2017/11/29/bandidos-en-barcelona/"><![CDATA[<p>I finally had the time to edit the clips that my friends and I filmed during our skate-trip to Barcelona two months ago. Do not expect big tricks, we wanted just to have fun cruising the around the streets. We had very good times, although the Catalonian crisis, people was still really friendly and smiling. Barcelona is really an amazing city!</p>

<div class="post-video">
  <iframe src="https://www.youtube.com/embed/2C-LwgQEsgw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Skateboarding" /><summary type="html"><![CDATA[I finally had the time to edit the clips that my friends and I filmed during our skate-trip to Barcelona two months ago. Do not expect big tricks, we wanted just to have fun cruising the around the streets. We had very good times, although the Catalonian crisis, people was still really friendly and smiling. Barcelona is really an amazing city!]]></summary></entry><entry><title type="html">2007-2017</title><link href="https://matteo-dunnhofer.github.io/2017/10/02/2007-2017/" rel="alternate" type="text/html" title="2007-2017" /><published>2017-10-02T14:41:32+02:00</published><updated>2017-10-02T14:41:32+02:00</updated><id>https://matteo-dunnhofer.github.io/2017/10/02/2007-2017</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2017/10/02/2007-2017/"><![CDATA[<p>This year my crew and I celebrate our first ten years on the skateboard. For the occasion we decided to produce some skateboard decks and so I had to take back the pencil and start drawing (like in the good old days). In the drawing it is represented, in a certain way, our “skateboarding path” from where we began to where we are now. You can see the result above, and the final printed board below. We are very happy with them.</p>

<p>Now we go skate!</p>

<div class="post-image">
  <img class="post-image" src="https://matteo-dunnhofer.github.io/assets/images/blog/2017-10-02/v1_cropgg.jpg" />
</div>

<div class="post-image">
  <img class="post-image" src="https://matteo-dunnhofer.github.io/assets/images/blog/2017-10-02/IMG_3736.jpg" />
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Skateboarding" /><category term="10 years" /><category term="pine" /><category term="skate" /><category term="skateboarding" /><category term="tarvisio" /><summary type="html"><![CDATA[This year my crew and I celebrate our first ten years on the skateboard. For the occasion we decided to produce some skateboard decks and so I had to take back the pencil and start drawing (like in the good old days). In the drawing it is represented, in a certain way, our “skateboarding path” from where we began to where we are now. You can see the result above, and the final printed board below. We are very happy with them.]]></summary></entry><entry><title type="html">My first Kaggle competition</title><link href="https://matteo-dunnhofer.github.io/2017/08/04/my-first-kaggle-competition/" rel="alternate" type="text/html" title="My first Kaggle competition" /><published>2017-08-05T00:08:44+02:00</published><updated>2017-08-05T00:08:44+02:00</updated><id>https://matteo-dunnhofer.github.io/2017/08/04/my-first-kaggle-competition</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2017/08/04/my-first-kaggle-competition/"><![CDATA[<p>I knew of the existence of Kaggle since I started studying Machine Learning. Unfortunately, during that time I did not have the experience nor the time to take part to any competition. But in last semester I made some good experiences that helped me to develop my skills in the field. At the same time, an interesting real world problem was published as a <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space">competition on the  Kaggle platform</a>. And so I realised that that was the best one to give myself a challenge.</p>

<div class="post-image">
  <img class="post-image" src="https://matteo-dunnhofer.github.io/assets/images/blog/2017-08-04/habitation1.jpg" />
  
  <p class="caption">
    An example of satellite image used in the competition. Credits: Planet and Kaggle.
  </p>
</div>

<p><a href="https://www.planet.com">Planet Labs</a> is a private company that owns the multitude of imaging-satellites that orbit around the Earth. These satellites form a constellation that can provide a complete image of our planet, all the time. This great amount of data is used by governments and  humanitarian, business, environmental (and many more) organisations to monitor the state and the changes of Earth’s ground. These stakeholders can also use the images to study the evolution or track the variations in different (maybe critic) areas of the planet, and exactly that is the context of the competition organised by Planet and its Brazilian partner SCCON: the deforestation of the Amazon rainforest. The seriousness of this problem (and it is really serious!) obviously grabs the attention of many researchers that use satellite data to track the rapid changes that occur inside the forest. But often its the huge vastness make the process of analysing the images really tedious, and a system that could automatically identify deforestation risk zones will certainly benefit the work of these people. These kind of procedures already exist but they do not for the images produced by Planet nor at an high level of resolution. So, Planet decided to challenge Kaggle’s users with the goal to develop a software tool able to classify the atmospheric condition and ground characteristics contained inside satellite images of the Amazon.</p>

<p>The dataset provided consisted of more than 40000 training examples. Every example (a chip image) was a 256px x 256px GeoTiff 4-band (red, green, blue, near infrared) image that covered an area of there forest of circa 90 hectares (950mt x 950mt circa). The associated labels explicated the weather condition (clear, cloudy, partly cloudy, haze) and the characteristics of the ground in that particular area (primary rainforest, water, habitation, agricolture, cultivation, road, bare ground, slash-and-burn cultivation, selective logging, blooming trees, conventional mines, artisanal mines and blow down). Every sample was classified with exactly one weather label and at least one (at most all) label for the ground. So, this happened to be a multi-label multi-class classification problem, instead of a classic multi-class classification. The labels were very unbalanced, making some classes really difficult to predict. Moreover, together with the GeoTiff images, Planet provided their RGB version for practice. Finally, the test set contained 61191 samples which labels had to be predicted. The metric used by Kaggle for the evaluation was the F2 score.</p>

<p>Given this good amount of training data, the use of Convolutional Neural Network (today the standard approach in Computer Vision) is very natural. To produce my solution I implemented different models of CNN, and <a href="https://arxiv.org/abs/1512.03385">ResNet</a> and <a href="https://arxiv.org/abs/1608.06993">DenseNet</a> (the latest ImageNet’s competition winners) were the ones that performed best. I trained both from-scratch and pretrained nets, the latter giving some improvements in the score. Although ImageNet does not contain this kind of satellite images, the features produced by pretrained models were more meaningful. In all the models, the last activation (usually a softmax) was replaced with a sigmoid. This was done to compute every class probability independently from the other classes, and so I could use labels as one-hot vectors with many ones as the number of classes present in the image. Classic tricks like data augmentation and normalisation, learning rate scheduling etc were performed.</p>

<p>The aim of Planet was to obtain models built on the GeoTiff images. Turned out from the experiments that the RGB images (provided only for practice) were more significant for the nets, and models trained on them performed definitely better than the one trained on the 4-band images. This was caused by a lot misclassifications in the latter.</p>

<p>For the predictions on the test set, instead of using the output of the single models, I implemented an ensemble of the best performing nets (that I evaluated on a validation set produced splitting the training set). In particular, I made ResNet34, ResNet101, DenseNet121, DenseNet169 and DenseNet201 vote for the labels of every test example. Then, classes that obtained a vote greater of equal to half of the number of models (in this case if a label was voted by at least three models) were retained. Before that, I also optimised the threshold of the class probabilities (through a brute force search), meaning that a class was predicted if its probability was greater than a particular value.</p>

<p>All the sources were developed in Python. For the models trained from scratch I used TensorFlow, to use the pretrained nets I employed PyTorch.</p>

<p>My solution obtained a final F2 score of 0.92942 that earned me the 88th position (inside the top 10%) and the bronze medal.</p>

<p>In conclusion, I am very happy for this first result and it has been a great and exciting experience from which a learned a lot of things. Among the others, it allowed me to study and implement complex and very effective CNNs, to face real world datasets (and they are not as the notorious you find in papers), to learn how ensembling can give a boost to prediction, and to discover the flexibility and simplicity of PyTorch.</p>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Software" /><category term="Amazon" /><category term="deep learning" /><category term="deforestation" /><category term="Kaggle" /><category term="machine learning" /><category term="rainforest" /><summary type="html"><![CDATA[I knew of the existence of Kaggle since I started studying Machine Learning. Unfortunately, during that time I did not have the experience nor the time to take part to any competition. But in last semester I made some good experiences that helped me to develop my skills in the field. At the same time, an interesting real world problem was published as a competition on the  Kaggle platform. And so I realised that that was the best one to give myself a challenge.]]></summary></entry><entry><title type="html">ASD Forest Park Crew membership registration</title><link href="https://matteo-dunnhofer.github.io/2017/06/20/asd-forest-park-crew-membership-registration/" rel="alternate" type="text/html" title="ASD Forest Park Crew membership registration" /><published>2017-06-20T13:54:49+02:00</published><updated>2017-06-20T13:54:49+02:00</updated><id>https://matteo-dunnhofer.github.io/2017/06/20/asd-forest-park-crew-membership-registration</id><content type="html" xml:base="https://matteo-dunnhofer.github.io/2017/06/20/asd-forest-park-crew-membership-registration/"><![CDATA[<p>Hello everyone!</p>

<p>During last spring I had the opportunity to do a little work for the ASD Forest Park Crew association (of which I am a member) that runs my hometown skatepark.</p>

<p>To access our skatepark a person must be a member of our association. Until last year, to become a fellow a person had to present himself at the skatepark, pay the registration fee and then get a paper card valid for the access to the park. A classic registration scenario. Starting from this season we decided to automate and digitize all this process. So, I realized a little web-app to manage the registration, the payment via PayPal and the generation of a PDF file that counts as a valid pass for the skatepark. With this “innovation” we are able to manage electronically and efficiently all the data about the association’s fellows and to reduce some managing costs.</p>

<p>You can check the app at this <a href="http://orso.io/forestpark/registration">page</a>. For every kind of info or to report a malfunction please send me an E-Mail.</p>

<p>We’ll wait for you at the skatepark 😉.</p>

<div class="post-image">
  <img class="post-image" src="https://matteo-dunnhofer.github.io/assets/images/blog/2017-06-20/park.jpg" />
  
  <p class="caption">
    Forest Park skatepark in Tarvisio.
  </p>
</div>]]></content><author><name>dontfollowmeimcrazy</name></author><category term="Software" /><category term="Forest Park" /><category term="registration" /><category term="Skateboard" /><category term="skatepark" /><category term="tarvisio" /><summary type="html"><![CDATA[Hello everyone!]]></summary></entry></feed>